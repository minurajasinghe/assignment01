function convergence_analysis_v2(solver_flag, fun, x0, guess_list1, guess_list2, filter_list)
    num_iter = length(guess_list1); %Number of times to run root finding for error plot

    % Define Abs Zero
    %rootywooty = secant_solver(fun, x0,x0+.001, 1e-14, 100);
    

    % Define global Variables
    global guess_list;
    guess_list = [];

    global x_regression;
    x_regression = [];
    global y_regression;
    y_regression = [];

    global id_list;
    id_list = [];
    global x_current_list;

    x_current_list = [];
    x_next_list = [];
    id_list = [];

    % x0_list = linspace(-5,5,num_iter);

    for n = 1:num_iter
        % x0 = guess_list1(n);%+ rand();
        % x1 = guess_list2(n);%+ rand();
        % x0 = x0_list(n);
        
        guess_list = [];
        
        % Bisection Method
        if (solver_flag == 1)
            root = bisection_solver(fun,guess_list1(n), guess_list2(n));
        % Newtons Method
        elseif (solver_flag == 2)
            root = newton_solver(fun,guess_list1(n));
            
        % Secant Method
        elseif (solver_flag == 3)
            [root, iterates] = secant_solver(fun,guess_list1(n), guess_list2(n), 1e-14, 100);
            
        % Fzero
        elseif (solver_flag == 4)
            root = fzero(fun,guess_list1(n));
        else
            disp("solver_flag must be 1 - 4")
        end
        
        if solver_flag == 3
            x_current_list = [x_current_list, iterates(1:end-1)];
            x_next_list    = [x_next_list, iterates(2:end)];
            id_list        = [id_list, 1:length(iterates)-1];
        else
            x_current_list = [x_current_list, guess_list(1:end-1)];
            x_next_list = [x_next_list, guess_list(2:end)];
            id_list = [id_list,1:length(guess_list)-1];
        end
        
    end
    global error_list0;
    global error_list1;

    error_list0 = abs(x_current_list - rootywooty);
    error_list1 = abs(x_next_list - rootywooty);
    
    figure;
    loglog(error_list0, error_list1,'ro','markerfacecolor','r','markersize',3); 
    ylim([1e-20, 1e3]); 
    xlabel("Error (n)")
    ylabel("Error (n+1)")
    if (solver_flag == 1)
        title("Bisection Method")
    % Newtons Method
    elseif (solver_flag == 2)
        title("Newton Method")
    % Secant Method
    elseif (solver_flag == 3)
        title("Secant Method")
    % Fzero
    elseif (solver_flag == 4)
        title("fzero")
    else
        disp("solver_flag must be 1 - 4")
    end
    hold on;
    

    x_regression = []; % e_n
    y_regression = []; % e_{n+1}

    %iterate through the collected data
    for n = 1:length(id_list)
        % if the error is not too big or too small
        % and it was enough iterations into the trial...
        if error_list0(n) > filter_list(1) && error_list0(n) < filter_list(2) && ...
           error_list1(n) > filter_list(3) && error_list1(n) < filter_list(4) && ...
           id_list(n)  > filter_list(5)
           % then add it to the set of points for regression
           x_regression(end+1) = error_list0(n);
           y_regression(end+1) = error_list1(n);
        end
    end


    loglog(x_regression, y_regression,'bo','MarkerFaceColor','b','MarkerSize',2);
    
    % Get error regression fit coefficients
    [ex_p,ex_k] = error_fit_coeffs(x_regression, y_regression);
    
    %generate x data on a logarithmic range
    fit_line_x = 10.^[-16:.01:1];
    %compute the corresponding y values
    fit_line_y = ex_k*fit_line_x.^ex_p;
    %plot on a loglog plot.
    loglog(fit_line_x, fit_line_y,'k-','linewidth',1)
    

    % % Bisection Method
    % if (solver_flag == 1)
    %     k = 0.5;
    %     p = 1;
    % % Newtons Method
    % elseif (solver_flag == 2)
    % 
    % % Secant Method
    % elseif (solver_flag == 3)
    % 
    % % Fzero
    % elseif (solver_flag == 4)
    % 
    % else
    %     disp("solver_flag must be 1 - 4")
    % end
    % Finite difference approximation to evaluate k
    [dfdx,d2fdx2] = approximate_derivative(fun, rootywooty);
    k = abs(.5*(d2fdx2/dfdx));



    
end